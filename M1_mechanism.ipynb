{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z0DIrI1fz0nP"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import random\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.cm import get_cmap\n",
    "import math\n",
    "import pickle\n",
    "\n",
    "rng = random.default_rng(seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dzQOlKMkl0vN"
   },
   "source": [
    "#Environment settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9YCUwnGQl1CX"
   },
   "outputs": [],
   "source": [
    "class Environment:\n",
    "    def __init__(self, settings):\n",
    "        self.discreteMap = settings['discreteMap']\n",
    "        self.sizeRechargeAreas = settings['sizeRechargeAreas']\n",
    "        self.rechargeValue = settings['rechargeValue']\n",
    "        self.rechargeAreas = settings['rechargeAreas']\n",
    "\n",
    "        #Save the recharge stations position and set a high value to them for plot reasons\n",
    "        RechargeAreaPositions = [[0 for i in range(20)] for j in range(20)]\n",
    "        station = 100\n",
    "        for i in range(len(self.rechargeAreas)):\n",
    "            RechargeAreaPositions[self.rechargeAreas[i][0]][self.rechargeAreas[i][1]] = station\n",
    "            RechargeAreaPositions[self.rechargeAreas[i][0]][self.rechargeAreas[i][1] + self.sizeRechargeAreas] = station\n",
    "            RechargeAreaPositions[self.rechargeAreas[i][0] + self.sizeRechargeAreas][self.rechargeAreas[i][1]] = station\n",
    "            RechargeAreaPositions[self.rechargeAreas[i][0] + self.sizeRechargeAreas][self.rechargeAreas[i][1] + self.sizeRechargeAreas] = station\n",
    "            station += 100\n",
    "\n",
    "        mat = np.matrix(RechargeAreaPositions)\n",
    "        with open('RechargeAreaPositions.txt','wb') as f:\n",
    "            for line in mat:\n",
    "                np.savetxt(f, line, fmt='%s', delimiter=\",\")\n",
    "        f.close()\n",
    "\n",
    "    def ifInRechargeArea(self, agentY, agentX, numVisitsRechargeArea):\n",
    "        i = 0\n",
    "        for rA in self.rechargeAreas:\n",
    "            if (agentY >= rA[0] and agentY <= (rA[0] + self.sizeRechargeAreas)) and (agentX >= rA[1] and agentX <= (rA[1] + self.sizeRechargeAreas)):\n",
    "                numVisitsRechargeArea[i] += 1\n",
    "                return self.rechargeValue[i]\n",
    "            i += 1\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b7npNxsA0LBz"
   },
   "source": [
    "#Robot settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3Zzf6KDVpzAP"
   },
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, env, actions, survive, distanceMeasure):\n",
    "        self.agentX = 0\n",
    "        self.agentY = 0\n",
    "        self.env = env\n",
    "        self.totalActions = len(actions)\n",
    "        self.totalFeatures = 8 #FEATURES\n",
    "        self.actions = actions\n",
    "        self.distanceMeasure = distanceMeasure\n",
    "\n",
    "        self.coordinates = []\n",
    "        self.numVisitsMap = [[0 for i in range(self.env.discreteMap)] for j in range(self.env.discreteMap)]\n",
    "        self.numVisitsMap_accumulated = [[0 for i in range(self.env.discreteMap)] for j in range(self.env.discreteMap)]\n",
    "\n",
    "        self.numVisitsRechargeArea = [0 for i in range(len(env.rechargeAreas))]\n",
    "\n",
    "        #BATTERY\n",
    "        self.homeostasisSurvive = survive['homeostasisSurvive']\n",
    "        self.maxEnergy = survive['maxEnergy']\n",
    "        self.minEnergy = survive['minEnergy']\n",
    "        self.discountEnergy = survive['discountEnergy']\n",
    "        self.energy = rng.integers(self.minEnergy, self.maxEnergy + 1)\n",
    "        \n",
    "        self.surviveDriveAll = []\n",
    "        self.energyAll = []\n",
    "        \n",
    "        #Fixed initial positions to use in the test phase\n",
    "        self.allY_test = [0, 0, 0, 0, 1, 1, 1, 2, 2, 3, 3, 3, 3, 4, 5, 5, 5, 6, 6, 6, 7, 7, 7, 8, 8, 9, 9, 9, 10, 10, 11, 12, 12, 13, 13, 13, 14, 14, 15, 16, 16, 16, 17, 17, 18, 19, 19, 19, 19, 19]\n",
    "        self.allX_test = [0, 4, 6, 19, 8, 12, 15, 3, 10, 1, 6, 12, 16, 18, 2, 5, 7, 4, 10, 13, 0, 15, 19, 7, 14, 5, 12, 16, 2, 10, 19, 1, 13, 5, 8, 16, 0, 3, 10, 1, 4, 17, 9, 13, 7, 0, 3, 11, 16, 19]\n",
    "        self.cxy = 0\n",
    "\n",
    "    def reset(self, test):\n",
    "        self.numVisitsRechargeArea = [0 for i in range(len(self.env.rechargeAreas))]        \n",
    "        self.energyAll = []\n",
    "        \n",
    "        if (test):\n",
    "            self.energy = self.homeostasisSurvive\n",
    "            self.agentX = self.allX_test[self.cxy]\n",
    "            self.agentY = self.allY_test[self.cxy]\n",
    "            self.cxy += 1\n",
    "        else:\n",
    "            self.energy = rng.integers(self.minEnergy, self.maxEnergy + 1)\n",
    "            self.agentX = rng.integers(0, self.env.discreteMap)\n",
    "            self.agentY = rng.integers(0, self.env.discreteMap)\n",
    "        \n",
    "            \n",
    "        currentDriveSurvive = self.computeDriveSurvive(self.energy)\n",
    "        \n",
    "        self.surviveDriveAll = []\n",
    "\n",
    "        self.coordinates = []\n",
    "        self.numVisitsMap = [[0 for i in range(self.env.discreteMap)] for j in range(self.env.discreteMap)]\n",
    "\n",
    "        minDist, up, down, left, right, station = self.closestRechargeArea(self.agentY, self.agentX)\n",
    "\n",
    "        return [currentDriveSurvive, minDist, up, down, left, right, self.agentY, self.agentX] #FEATURES\n",
    "\n",
    " \n",
    "    #DRIVE SURVIVE\n",
    "    def computeDriveSurvive(self, sensor):\n",
    "        return -(self.homeostasisSurvive - sensor)\n",
    "\n",
    "    #REWARD FUNCTIONS\n",
    "    def rewardFunction(self, currentDriveSurvive):\n",
    "        if ((abs(int(currentDriveSurvive)) == 0) and (currentDriveSurvive * (-1) <= 0)):\n",
    "            return 1\n",
    "        elif (currentDriveSurvive < 0):\n",
    "            return currentDriveSurvive \n",
    "        else:\n",
    "            return -(currentDriveSurvive * 0.5) \n",
    "\n",
    "\n",
    "    def computeDriveSurviveReward(self, currentDriveSurvive):\n",
    "        return self.rewardFunction(currentDriveSurvive)\n",
    "\n",
    "    def step(self, action):\n",
    "        #MOVE\n",
    "        # Up \n",
    "        if (action == 1):\n",
    "            self.agentY += 1\n",
    "            if self.agentY > self.env.discreteMap-1: self.agentY = self.env.discreteMap-1\n",
    "\n",
    "        # Down\n",
    "        elif (action == 2):\n",
    "            self.agentY -= 1\n",
    "            if self.agentY < 0: self.agentY = 0\n",
    "\n",
    "        # Left\n",
    "        elif (action == 3):\n",
    "            self.agentX -=1\n",
    "            if self.agentX < 0: self.agentX = 0\n",
    "\n",
    "        # Right \n",
    "        elif (action == 4):\n",
    "            self.agentX += 1\n",
    "            if self.agentX > self.env.discreteMap-1: self.agentX = self.env.discreteMap-1\n",
    "\n",
    "        self.coordinates.append([self.agentX, self.agentY])\n",
    "        self.numVisitsMap[self.agentY][self.agentX] += 1\n",
    "\n",
    "        self.numVisitsMap_accumulated[self.agentY][self.agentX] += 1#all visits to each station during the training phase\n",
    "\n",
    "        #UPDATE BATTERY STUFF\n",
    "        self.energy -= self.discountEnergy\n",
    "        recharge = self.checkIfRecharge()\n",
    "        self.energy = min(self.maxEnergy, self.energy + recharge)\n",
    "        self.energyAll.append(self.energy)\n",
    "\n",
    "        #Compute drives and reward\n",
    "        currentDriveSurvive = self.computeDriveSurvive(self.energy)\n",
    "        rewardSurvive = self.computeDriveSurviveReward(currentDriveSurvive)\n",
    "        self.surviveDriveAll.append(currentDriveSurvive)\n",
    "\n",
    "        totalReward = rewardSurvive\n",
    "\n",
    "        done = self.death()\n",
    "\n",
    "        minDist, up, down, left, right, station = self.closestRechargeArea(self.agentY, self.agentX)\n",
    "\n",
    "        new_state = [currentDriveSurvive, minDist, up, down, left, right, self.agentY, self.agentX]\n",
    "\n",
    "        return new_state, totalReward, done \n",
    "    \n",
    "    def death(self):\n",
    "        if self.energy <= self.minEnergy:\n",
    "            return 1\n",
    "        return 0\n",
    "\n",
    "    def checkIfRecharge(self):\n",
    "        recharge = self.env.ifInRechargeArea(self.agentY, self.agentX, self.numVisitsRechargeArea)\n",
    "        return recharge\n",
    "    \n",
    "    def manhattanDistance(self, agentY, agentX, middleY_RA, middleX_RA):\n",
    "        return abs(agentX - middleX_RA) + abs(agentY - middleY_RA)\n",
    "\n",
    "    def euclideanDistance(self, agentY, agentX, middleY_RA, middleX_RA):\n",
    "        return math.sqrt(((agentX - middleX_RA) ** 2) + ((agentY - middleY_RA) ** 2))\n",
    "    \n",
    "    def closestRechargeArea(self, agentY, agentX):\n",
    "        minDist = 99999\n",
    "        station = 0\n",
    "        i = 0\n",
    "        \n",
    "        for rA in self.env.rechargeAreas:\n",
    "            middleX_RA = rA[1] + self.env.sizeRechargeAreas/2\n",
    "            middleY_RA = rA[0] + self.env.sizeRechargeAreas/2\n",
    "            if (self.distanceMeasure == 'Euclidean'):\n",
    "                dist = self.euclideanDistance(agentY, agentX, middleY_RA, middleX_RA)\n",
    "            else:\n",
    "                dist = self.manhattanDistance(agentY, agentX, middleY_RA, middleX_RA)\n",
    "\n",
    "            if ((self.distanceMeasure == 'Manhattan' and dist <= self.env.sizeRechargeAreas) or (self.distanceMeasure == 'Euclidean' and dist < self.env.sizeRechargeAreas)):#fiz isso pq to usando o ponto central da estação, mas se dist for menor que sizerecharge quer dizer que ja esta na estação (mas nao no meio dela)\n",
    "                dist = 0#already at the station\n",
    "\n",
    "            if dist < minDist:\n",
    "                minDist = dist\n",
    "                up = 0\n",
    "                down = 0\n",
    "                left = 0\n",
    "                right = 0\n",
    "                station = i\n",
    "\n",
    "                horizon = agentX - middleX_RA \n",
    "                vertical = agentY - middleY_RA\n",
    "\n",
    "                if(agentX < rA[1] and (horizon != self.env.sizeRechargeAreas/2)):\n",
    "                    right = 1\n",
    "                elif(agentX > rA[1] and (horizon != self.env.sizeRechargeAreas/2)):\n",
    "                    left = 1\n",
    "\n",
    "                if(agentY > rA[0] and (vertical != self.env.sizeRechargeAreas/2)):\n",
    "                    down = 1\n",
    "                elif(agentY < rA[0] and (vertical != self.env.sizeRechargeAreas/2)):\n",
    "                    up = 1\n",
    "            i += 1\n",
    "\n",
    "        return minDist, up, down, left, right, station\n",
    "\n",
    "    def save(self, sensor, fileName):\n",
    "        mat = np.matrix(sensor)\n",
    "        with open(fileName,'wb') as f:\n",
    "            for line in mat:\n",
    "                np.savetxt(f, line, fmt='%s', delimiter=\",\")\n",
    "        f.close()\n",
    "\n",
    "    def saveSensorsData(self, expIDX):\n",
    "        self.save(self.energyAll, 'Energy' + str(expIDX) + '.txt')\n",
    "        self.save(self.surviveDriveAll, 'SurviveDrive' + str(expIDX) + '.txt')\n",
    "        self.save(self.numVisitsRechargeArea, 'NumVisitsRechargeArea' + str(expIDX) + '.txt')\n",
    "        self.save(self.numVisitsMap, 'NumVisitsMap_Test' + str(expIDX) + '.txt')\n",
    "        self.save(self.coordinates, 'Coordinates' + str(expIDX) + '.txt')\n",
    "\n",
    "    def dataTrain_Visits(self):\n",
    "        self.save(self.numVisitsMap_accumulated, 'NumVisitsMap_TrainAccumulated0.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SJDF1onT0Ryx"
   },
   "source": [
    "#Q-Learning Function Approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YgZsy86tz32o"
   },
   "outputs": [],
   "source": [
    "class ApproximateQAgent:\n",
    "    def __init__(self, robot, learning_parameters, exploration_parameters, glie):\n",
    "        self.robot = robot \n",
    "\n",
    "        # learning parameters (dict)\n",
    "        self.alpha = learning_parameters['alpha']#learning rate\n",
    "        self.gamma = learning_parameters['gamma'] #discount factor\n",
    "\n",
    "        # exploration parameters\n",
    "        self.epsilon = exploration_parameters['epsilon']\n",
    "        self.epsilon_min = exploration_parameters['epsilon_min']\n",
    "        self.epsilon_decay = exploration_parameters['epsilon_decay']\n",
    "        self.glie = glie\n",
    "\n",
    "        self.featuresS = 0\n",
    "        self.featuresSL = 0\n",
    "\n",
    "        self.featuresPerAction = 0\n",
    "\n",
    "        self.independFeatures = self.robot.totalFeatures - 2 #All features, except X and Y\n",
    "\n",
    "################################## FILES #################################################\n",
    "\n",
    "    def recoverWeights(self):\n",
    "        with open('FeaturesPerAction_weightsLast.txt', 'r') as f:\n",
    "            self.featuresPerAction = [[float(num) for num in line.split(',')] for line in f]\n",
    "        f.close()\n",
    "\n",
    "    def saveWeights(self, fileName, data):\n",
    "        mat = np.matrix(data)\n",
    "        with open(fileName + '.txt','wb') as f:\n",
    "            for line in mat:\n",
    "                np.savetxt(f, line, fmt='%s', delimiter=\",\")\n",
    "        f.close()\n",
    "\n",
    "    def saveDataTraining(self, episode_rewards, episode_steps):\n",
    "        z = zip(episode_rewards, episode_steps)\n",
    "        f = open('RewardsLearning.txt', 'w')\n",
    "        for t in z:\n",
    "            line = ' '.join(str(x) for x in t)\n",
    "            f.write(line + '\\n')\n",
    "        f.close()\n",
    "\n",
    "    def saveBestReward(self, bestEpisode, episode_rewards):\n",
    "        best = [bestEpisode, episode_rewards[bestEpisode]]\n",
    "        print(best)\n",
    "        with open('BestEpisodeReward.txt','wb') as f:\n",
    "            np.savetxt(f, best, fmt='%s', delimiter=\",\")\n",
    "        f.close()\n",
    "\n",
    "################################## Qlearning #################################################\n",
    "\n",
    "    def init_featuresWeight(self):\n",
    "        self.featuresPerAction = [[rng.random() * 0.01 for i in range(self.independFeatures + (self.robot.totalFeatures - self.independFeatures) * self.robot.env.discreteMap)] for j in range(self.robot.totalActions)] \n",
    "\n",
    "\n",
    "    def setFeatures_binario(self, stateFeatures_, state_S_or_SL):\n",
    "        size = self.robot.env.discreteMap\n",
    "        stateFeatures = np.zeros(self.independFeatures + (self.robot.totalFeatures - self.independFeatures) * self.robot.env.discreteMap)\n",
    "        for i in range(self.independFeatures):\n",
    "            stateFeatures[i] = stateFeatures_[i]#currentDriveSurvive, minDist, up, down, left, right\n",
    "\n",
    "        inter = np.zeros(size)\n",
    "        j = 0\n",
    "        for i in range(self.independFeatures, len(stateFeatures_)):#agentY, agentX\n",
    "            inter[stateFeatures_[i]] = 1\n",
    "            stateFeatures[j * size + self.independFeatures : (j+1) * size + self.independFeatures] = inter\n",
    "            inter[stateFeatures_[i]] = 0\n",
    "            j += 1\n",
    "\n",
    "        if state_S_or_SL == 0:\n",
    "            self.featuresS = stateFeatures\n",
    "        else:\n",
    "            self.featuresSL = stateFeatures\n",
    "\n",
    "    def getFeatures(self, state_S_or_SL):\n",
    "        if state_S_or_SL == 0:\n",
    "            return self.featuresS\n",
    "        else:\n",
    "            return self.featuresSL\n",
    "\n",
    "    def getQvalue(self, actionIndex, state_S_or_SL):\n",
    "        qValue = 0\n",
    "        features = self.getFeatures(state_S_or_SL)\n",
    "\n",
    "        for i in range(self.independFeatures + (self.robot.totalFeatures - self.independFeatures) * self.robot.env.discreteMap):\n",
    "            qValue += features[i] * self.featuresPerAction[actionIndex][i]\n",
    "\n",
    "        return qValue\n",
    "  \n",
    "    def getMaxQValue(self):\n",
    "        maxQinSL = self.getQvalue(0, 1)#I assume that the first is the best\n",
    "        for i in range(1, self.robot.totalActions):\n",
    "            value = self.getQvalue(i, 1)\n",
    "            if value > maxQinSL:\n",
    "                maxQinSL = value\n",
    "        return maxQinSL\n",
    "\n",
    "    def update(self, actionIdid, reward):\n",
    "        Q_sa = self.getQvalue(actionIdid, 0)\n",
    "        Max_Qsl = self.getMaxQValue()\n",
    "        TD_target = reward + self.gamma * Max_Qsl\n",
    "        for i in range(self.independFeatures + (self.robot.totalFeatures - self.independFeatures) * self.robot.env.discreteMap):\n",
    "            self.featuresPerAction[actionIdid][i] += self.alpha * ((TD_target - Q_sa) * self.featuresS[i])\n",
    "\n",
    "        self.featuresS = self.featuresSL[:]\n",
    "\n",
    "    def updateEpsilon(self, totalEpisodes):\n",
    "        if self.glie == 'linear':\n",
    "            self.epsilon = max(self.epsilon_min, self.epsilon - (1/totalEpisodes))\n",
    "        elif self.glie == 'exponential':\n",
    "            self.epsilon = max(self.epsilon_min, self.epsilon * (1 - self.epsilon_decay))\n",
    "        elif self.glie == 'constant':\n",
    "            self.epsilon = self.epsilon\n",
    "\n",
    "    def getAction(self):\n",
    "        if rng.random() < self.epsilon:\n",
    "          # exploration, random choice\n",
    "            action = rng.integers(0,self.robot.totalActions)\n",
    "            selectedAction = action\n",
    "        else:\n",
    "          # exploitation, max value for given state\n",
    "            selectedAction = 0 #Assume that the first is the better\n",
    "            maxQinSL = self.getQvalue(0, 0)\n",
    "\n",
    "            for i in range(1, self.robot.totalActions):\n",
    "                value = self.getQvalue(i, 0)\n",
    "                if value > maxQinSL:\n",
    "                    maxQinSL = value\n",
    "                    selectedAction = i\n",
    "\n",
    "        return selectedAction\n",
    "\n",
    "################################## TRAIN #################################################\n",
    "\n",
    "    def learn(self, max_steps = 5000, total_episodes = 25000):\n",
    "        self.init_featuresWeight()\n",
    "        self.episode_rewards = np.zeros(total_episodes)\n",
    "        self.episode_steps = np.zeros(total_episodes)  \n",
    "        bestEpisode = 0\n",
    "        for episode in range(total_episodes):\n",
    "            state = self.robot.reset(0)\n",
    "            self.setFeatures_binario(state[:], 0)\n",
    "            for step in range(max_steps):\n",
    "                action = self.getAction()\n",
    "                new_state, reward, done = self.robot.step(action)\n",
    "                self.setFeatures_binario(new_state[:], 1)\n",
    "                self.update(action, reward)\n",
    "                self.episode_rewards[episode] += reward\n",
    "                if done:  \n",
    "                    print(\"died episode \", episode ,  \" \" , np.sum(robot.numVisitsRechargeArea), \"reward:\", self.episode_rewards[episode])\n",
    "                    break\n",
    "            if (done == 0):\n",
    "                print(\"episode \", episode ,  \" \" ,  np.sum(robot.numVisitsRechargeArea), \"reward:\", self.episode_rewards[episode])  \n",
    "            self.episode_steps[episode] = step + 1\n",
    "            self.updateEpsilon(total_episodes)\n",
    "            if (episode > 500 and (self.episode_rewards[episode]/self.episode_steps[episode] >= self.episode_rewards[bestEpisode]/self.episode_steps[bestEpisode])):\n",
    "                self.saveWeights('FeaturesPerAction_weightsBEST', self.featuresPerAction)\n",
    "                bestEpisode = episode\n",
    "            if (episode % 1000 == 0):\n",
    "                self.saveDataTraining(self.episode_rewards, self.episode_steps)\n",
    "                self.saveBestReward(bestEpisode, self.episode_rewards)\n",
    "                self.robot.dataTrain_Visits()\n",
    "        self.saveWeights('FeaturesPerAction_weightsLast', self.featuresPerAction)\n",
    "        self.saveDataTraining(self.episode_rewards, self.episode_steps)\n",
    "        self.saveBestReward(bestEpisode, self.episode_rewards)\n",
    "        self.robot.dataTrain_Visits()\n",
    "\n",
    "################################## TEST #################################################\n",
    "\n",
    "    def evaluate(self, max_steps, expIDX):\n",
    "        self.epsilon = 0\n",
    "        self.recoverWeights()\n",
    "        state = self.robot.reset(1)\n",
    "        self.setFeatures_binario(state[:], 0)\n",
    "        for step in range(max_steps):\n",
    "            action = self.getAction()\n",
    "            new_state, reward, done = self.robot.step(action)\n",
    "            self.setFeatures_binario(new_state[:], 0)\n",
    "            if done:\n",
    "                print(\"died EXPLORING \" , step, \" steps\")\n",
    "                break\n",
    "        self.robot.saveSensorsData(expIDX)\n",
    "\n",
    "################################## ONLY ANALYZE ACTION TO CHOSE #################################################\n",
    "\n",
    "    def chooseActionPerPosition(self,energy):\n",
    "        self.epsilon = 0\n",
    "        self.recoverWeights()\n",
    "\n",
    "        actions = [[-1  for i in range(self.robot.env.discreteMap)] for j in range(self.robot.env.discreteMap)]\n",
    "\n",
    "        energyDrive = self.robot.computeDriveSurvive(energy)\n",
    "\n",
    "        for i in range(self.robot.env.discreteMap):\n",
    "            for j in range(self.robot.env.discreteMap):\n",
    "                minDist, up, down, left, right, station = self.robot.closestRechargeArea(i, j)\n",
    "                state = [energyDrive, minDist, up, down, left, right, i, j]\n",
    "                self.setFeatures_binario(state[:], 0)\n",
    "                action = self.getAction()\n",
    "                actions[i][j] = action\n",
    "        self.saveWeights('ActionsPerPosition_Energy' + str(energy), actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3jLbvDC5xSz5"
   },
   "source": [
    "#Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f2bbZUAwX6ou"
   },
   "outputs": [],
   "source": [
    "################################## PLOTS #################################################\n",
    "class dataVisualization:\n",
    "    def __init__(self, settings):\n",
    "        self.discreteMap = settings['discreteMap']\n",
    "        self.rechargeAreas = settings['rechargeAreas']\n",
    "        self.sizeRechargeAreas = settings['sizeRechargeAreas']\n",
    "        self.labelsRechargeArea = settings['labelsRechargeArea']\n",
    "        #self.colors_rechargeAreas = ['tab:blue', 'tab:orange', 'tab:green',  'tab:red']\n",
    "        self.colors_rechargeAreas = ['red', 'red', 'red',  'red']\n",
    "        self.cmap = ['#440154FF','tab:blue', 'tab:orange', 'tab:green',  'tab:red']\n",
    "        \n",
    "        \n",
    "    def show_heatmap(self, fileName, numExps, annot=False):\n",
    "        numVisitsMap = [[0 for i in range(self.discreteMap)] for j in range(self.discreteMap)]\n",
    "        for i in range(numExps):\n",
    "            with open(fileName+ str(i) + '.txt', 'r') as f:\n",
    "                best_q_mat = [[int(num) for num in line.split(',')] for line in f]\n",
    "            f.close()\n",
    "            best_q_mat = np.matrix(best_q_mat)\n",
    "            numVisitsMap = numVisitsMap + best_q_mat\n",
    "\n",
    "        average = numVisitsMap/numExps \n",
    "        fig, ax = plt.subplots(figsize=(15,15))\n",
    "        cmap = get_cmap()\n",
    "\n",
    "        ax = sns.heatmap(average, annot=annot, linewidths=.3, cmap=cmap, fmt=\".0f\", linecolor=\"grey\")\n",
    "\n",
    "        for i in range(len(self.rechargeAreas)):\n",
    "            x = []\n",
    "            y = []\n",
    "            x.append(self.rechargeAreas[i][1])\n",
    "            y.append(self.rechargeAreas[i][0])\n",
    "\n",
    "            x.append(self.rechargeAreas[i][1] + (2 * self.sizeRechargeAreas))\n",
    "            y.append(self.rechargeAreas[i][0])\n",
    "\n",
    "            x.append(self.rechargeAreas[i][1] + (2 * self.sizeRechargeAreas))\n",
    "            y.append(self.rechargeAreas[i][0] + (2 * self.sizeRechargeAreas))\n",
    "\n",
    "            x.append(self.rechargeAreas[i][1])\n",
    "            y.append(self.rechargeAreas[i][0] + (2 * self.sizeRechargeAreas))\n",
    "\n",
    "            x.append(self.rechargeAreas[i][1])\n",
    "            y.append(self.rechargeAreas[i][0])\n",
    "\n",
    "            ax.plot(x, y, color = self.colors_rechargeAreas[i])\n",
    "\n",
    "        ax.xaxis.tick_top()\n",
    "        ax.xaxis.set_label_position('top')\n",
    "        plt.xlabel(\"X\")\n",
    "        plt.ylabel(\"Y\")\n",
    "        plt.title(fileName, pad = 20)\n",
    "        plt.savefig(fileName + '.pdf', dpi = 400, bbox_inches='tight')\n",
    "        plt.savefig(fileName + '.png', dpi = 400, bbox_inches='tight')\n",
    "        plt.show()\n",
    "\n",
    "    def plot_actionsPerPosition(self, value, annot=False,):\n",
    "        #Get RechargeArea positions\n",
    "        with open('RechargeAreaPositions.txt', 'r') as f:\n",
    "            best_q_mat = [[int(num) for num in line.split(',')] for line in f]\n",
    "        f.close()\n",
    "        best_q_mat = np.matrix(best_q_mat)\n",
    "\n",
    "        #Plot RechargeArea positions\n",
    "        fig, ax = plt.subplots(figsize=(10,10))\n",
    "        ax = sns.heatmap(best_q_mat, annot=annot, linewidths=.3, cmap=self.cmap, fmt=\".0f\", cbar=False, linecolor=\"grey\")\n",
    "    \n",
    "        #Get and Plot the action chose in each position\n",
    "        with open('ActionsPerPosition_Energy' + str(value) + '.txt', 'r') as f:\n",
    "            actions = [[int(num) for num in line.split(',')] for line in f]\n",
    "        f.close()\n",
    "        #print(actions)\n",
    "        for i in range(self.discreteMap):#tamanho do environment\n",
    "            for j in range(self.discreteMap):#tamanho do environment\n",
    "                if(actions[i][j] == 0):\n",
    "                    ax.scatter(j+0.5, i+0.5, marker='o', s=100, color='#000000')\n",
    "                elif(actions[i][j] == 1):\n",
    "                    ax.scatter(j+0.5, i+0.5, marker='v', s=100, color='#FEE100')\n",
    "                elif(actions[i][j] == 2):\n",
    "                    ax.scatter(j+0.5, i+0.5, marker='^', s=100, color='#FFFFFF')\n",
    "                elif(actions[i][j] == 3):\n",
    "                    ax.scatter(j+0.5, i+0.5, marker='<', s=100, color='#00E9E7')\n",
    "                elif(actions[i][j] == 4):\n",
    "                    ax.scatter(j+0.5, i+0.5, marker='>', s=100, color='#E2C1F3')\n",
    "            \n",
    "        ax.xaxis.tick_top()\n",
    "        ax.xaxis.set_label_position('top')\n",
    "        plt.xlabel(\"X\")\n",
    "        plt.ylabel(\"Y\")\n",
    "        # Only y-axis labels need their rotation set, x-axis labels already have a rotation of 0\n",
    "        _, labels = plt.yticks()\n",
    "        plt.setp(labels, rotation=0)\n",
    "\n",
    "        plt.title('Actions Per Position_Energy' + str(value))\n",
    "        plt.savefig('ActionsPerPosition_Energy' + str(value) +'.pdf', dpi = 400, bbox_inches='tight')\n",
    "        plt.savefig('ActionsPerPosition_Energy' + str(value) +'.png', dpi = 400, bbox_inches='tight')\n",
    "        plt.show()\n",
    "\n",
    "    def plotTotalRewardsPerEisode(self, timeWindow):\n",
    "        rewards = []\n",
    "        actions = []\n",
    "        with open('RewardsLearning.txt') as f:\n",
    "            for line in f:\n",
    "                x = line.replace('\\n', '').split(\" \")\n",
    "                rewards.append(float(x[0]))\n",
    "                actions.append(int(float(x[1])))\n",
    "        f.close()\n",
    "\n",
    " \n",
    "        timeWindow_RewardValues = []\n",
    "        timeWindow_ActionValues = []\n",
    "        for i in range(len(rewards)//timeWindow):\n",
    "            values = rewards[i * timeWindow: i*timeWindow + timeWindow]\n",
    "            timeWindow_RewardValues.append(values)\n",
    "            values = actions[i * timeWindow: i*timeWindow + timeWindow]\n",
    "            timeWindow_ActionValues.append(values)\n",
    "\n",
    "        RewardAverage_TimeWindow = []\n",
    "        ActionsAverage_TimeWindow = []\n",
    "        stdReward = []\n",
    "        stdActions = []\n",
    "        steps = []\n",
    "\n",
    "        for i in range(len(rewards)//timeWindow):\n",
    "            RewardAverage_TimeWindow.append(np.mean(timeWindow_RewardValues[i], dtype=np.float64))\n",
    "            stdReward.append(np.std(timeWindow_RewardValues[i], dtype=np.float64))\n",
    "            ActionsAverage_TimeWindow.append(np.mean(timeWindow_ActionValues[i], dtype=np.float64))\n",
    "            stdActions.append(np.std(timeWindow_ActionValues[i], dtype=np.float64))\n",
    "            steps.append(i)\n",
    "        \n",
    "                \n",
    "        rM = []\n",
    "        for i in range(len(RewardAverage_TimeWindow)):\n",
    "            rM.append(RewardAverage_TimeWindow[i]/ActionsAverage_TimeWindow[i])\n",
    "\n",
    "        fig, ax = plt.subplots(nrows=2, sharex=True)\n",
    "        plt.xlabel('Episodes')\n",
    "        ax[0].set_ylabel('Average Reward')\n",
    "        ax[1].set_ylabel('Steps')\n",
    "\n",
    "        ax[0].plot(steps, rM)\n",
    "        ax[1].plot(steps, ActionsAverage_TimeWindow)\n",
    "        ax[1].fill_between(steps, [elemA + elemB for elemA, elemB in zip(ActionsAverage_TimeWindow, stdActions)] , [elemA - elemB for elemA, elemB in zip(ActionsAverage_TimeWindow, stdActions)], alpha=0.2)\n",
    "  \n",
    "        plt.savefig('TotalRewards_AverageAndAction.pdf', dpi = 400, bbox_inches='tight')\n",
    "        plt.savefig('TotalRewards_AverageAndAction.png', dpi = 400, bbox_inches='tight')\n",
    "        plt.show()\n",
    "\n",
    "    def plotSensorsData(self, sensorFile, labelX, numExps):\n",
    "        allSensorsData_Average = []\n",
    "        allSensorsData_std = []\n",
    "        numberOfActions = []\n",
    "        fig, ax = plt.subplots()\n",
    "\n",
    "        for i in range(numExps):\n",
    "            with open(sensorFile+ str(i) + '.txt') as f:\n",
    "                for line in f:\n",
    "                    x = line.replace('\\n', '').split(\",\")\n",
    "            f.close()\n",
    "            sensorData = [float(k) for k in x]\n",
    "            numberOfActions.append(len(sensorData))\n",
    "            allSensorsData_Average.append(np.mean(sensorData, dtype=np.float64))\n",
    "            allSensorsData_std.append(np.std(sensorData, dtype=np.float64))\n",
    "\n",
    "        steps = []\n",
    "        for i in range(numExps):\n",
    "            steps.append(i+1)\n",
    "        plt.title('Average ' + labelX + ' per Experiment')\n",
    "        plt.xlabel(\"Experiments\")\n",
    "\n",
    "        ax.plot(steps, allSensorsData_Average)\n",
    "        ax.fill_between(steps, [elemA + elemB for elemA, elemB in zip(allSensorsData_Average, allSensorsData_std)] , [elemA - elemB for elemA, elemB in zip(allSensorsData_Average, allSensorsData_std)], alpha=0.2)\n",
    "\n",
    "        plt.savefig(labelX + '_.pdf', dpi = 400, bbox_inches='tight')\n",
    "        plt.savefig(labelX + '_.png', dpi = 400, bbox_inches='tight')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_tY0Lp-6qH3w"
   },
   "source": [
    "#Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KoK7D75MqRyW"
   },
   "source": [
    "##Start the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7tTi1XqLqQ7Y"
   },
   "outputs": [],
   "source": [
    "settings = {\n",
    "    'discreteMap': 20,\n",
    "    'sizeRechargeAreas': 1,\n",
    "    'labelsRechargeArea': ['A', 'B', 'C', 'D'],\n",
    "    'rechargeAreas': [[2,4], [4,13], [16, 2], [14,15]],#[yInicial, XInicial]\n",
    "    'rechargeValue': [3, 3, 3, 3]#1, 4, 3, 2\n",
    "}\n",
    "\n",
    "env = Environment(settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B4JzeSYH2uiF"
   },
   "outputs": [],
   "source": [
    "#Need: Energy, Drive: Survive\n",
    "survive = {\n",
    "    'homeostasisSurvive': 30,\n",
    "    'maxEnergy': 50,\n",
    "    'minEnergy': 0,\n",
    "    'discountEnergy': 0.1\n",
    "}\n",
    "\n",
    "actions = {\n",
    "    0: 'Stop',\n",
    "    1: 'Up',\n",
    "    2: 'Down',\n",
    "    3: 'Left',\n",
    "    4: 'Right'\n",
    "}\n",
    " \n",
    "distanceMeasure = ['Euclidean', 'Manhattan']\n",
    "\n",
    "robot = Agent(env, actions, survive, distanceMeasure[0])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ih0X8udE08G1"
   },
   "outputs": [],
   "source": [
    "# q-learning parameters\n",
    "learning_parameters = {\n",
    "    'alpha': 0.0001,\n",
    "    'gamma': 0.9\n",
    "} \n",
    "# exploration-exploitation parameters\n",
    "exploration_parameters = {\n",
    "    'epsilon': 1.0, #exploration probability at start\n",
    "    'epsilon_min': 0.01, #minimum exploration probability\n",
    "    'epsilon_decay': 0.0003  #exponential decay rate for exploration prob\n",
    "}\n",
    "\n",
    "glie = ['linear', 'exponential', 'constant']\n",
    "qApp_Agent_1 = ApproximateQAgent(robot, learning_parameters, exploration_parameters, glie[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sdovEZ6LA_Q2"
   },
   "outputs": [],
   "source": [
    "max_steps = 5000\n",
    "total_episodes = 25000\n",
    "qApp_Agent_1.learn(max_steps, total_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6iOvqdIWlSMQ"
   },
   "outputs": [],
   "source": [
    "max_steps = 8000\n",
    "numExps = 50\n",
    "for i in range(numExps):\n",
    "    qApp_Agent_1.evaluate(max_steps, i)\n",
    "    print('End of the experiment ', i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0YEcHz26i0nx"
   },
   "outputs": [],
   "source": [
    "numExps = 50\n",
    "max_steps = 8000\n",
    "\n",
    "p = dataVisualization(settings)\n",
    "timeWindow = 100\n",
    "\n",
    "p.plotTotalRewardsPerEisode(timeWindow)\n",
    "p.show_heatmap('NumVisitsMap_TrainAccumulated', 1)\n",
    "p.show_heatmap('NumVisitsMap_Test', numExps)\n",
    "\n",
    "p.plotSensorsData('Energy', 'Energy', numExps)\n",
    "p.plotSensorsData('SurviveDrive', 'Drive Survive', numExps)\n",
    "\n",
    "energy = [15, 30, 45]\n",
    "for i in range(len(energy)):\n",
    "    qApp_Agent_1.chooseActionPerPosition(energy[i])\n",
    "    p.plot_actionsPerPosition(energy[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Energy_Learner.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
