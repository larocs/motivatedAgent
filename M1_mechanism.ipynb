{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Z0DIrI1fz0nP"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import random\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.cm import get_cmap\n",
    "from matplotlib.collections import LineCollection\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "import math\n",
    "import pickle\n",
    "\n",
    "rng = random.default_rng(seed = 42) #seeds used = [42, 11, 37, 59, 17]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "dzQOlKMkl0vN"
   },
   "source": [
    "#Environment settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "9YCUwnGQl1CX"
   },
   "outputs": [],
   "source": [
    "rng = random.default_rng(seed = 42)\n",
    "\n",
    "class Environment:\n",
    "    def __init__(self, settings):\n",
    "        self.discreteMap = settings['discreteMap']\n",
    "        self.sizeRechargeAreas = settings['sizeRechargeAreas']\n",
    "        self.rechargeValue = settings['rechargeValue']\n",
    "        self.rechargeAreas = settings['rechargeAreas']\n",
    "\n",
    "        #Save the recharge stations position and set a high value to them for plot reasons\n",
    "        RechargeAreaPositions = [[0 for i in range(20)] for j in range(20)]\n",
    "        station = 100\n",
    "        for i in range(len(self.rechargeAreas)):\n",
    "            RechargeAreaPositions[self.rechargeAreas[i][0]][self.rechargeAreas[i][1]] = station\n",
    "            RechargeAreaPositions[self.rechargeAreas[i][0]][self.rechargeAreas[i][1] + self.sizeRechargeAreas] = station\n",
    "            RechargeAreaPositions[self.rechargeAreas[i][0] + self.sizeRechargeAreas][self.rechargeAreas[i][1]] = station\n",
    "            RechargeAreaPositions[self.rechargeAreas[i][0] + self.sizeRechargeAreas][self.rechargeAreas[i][1] + self.sizeRechargeAreas] = station\n",
    "            station += 100\n",
    "\n",
    "        mat = np.matrix(RechargeAreaPositions)\n",
    "        with open('RechargeAreaPositions.txt','wb') as f:\n",
    "            for line in mat:\n",
    "                np.savetxt(f, line, fmt='%s', delimiter=\",\")\n",
    "        f.close()\n",
    "\n",
    "    def ifInRechargeArea(self, agentY, agentX, numVisitsRechargeArea, visitsRechargeAreaEpisode, time):\n",
    "        i = 0\n",
    "        for rA in self.rechargeAreas:\n",
    "            if (agentY >= rA[0] and agentY <= (rA[0] + self.sizeRechargeAreas)) and (agentX >= rA[1] and agentX <= (rA[1] + self.sizeRechargeAreas)):\n",
    "                numVisitsRechargeArea[i] += 1\n",
    "                visitsRechargeAreaEpisode.append([time, i])\n",
    "                return self.rechargeValue[i]\n",
    "            i += 1\n",
    "        return 0\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "b7npNxsA0LBz"
   },
   "source": [
    "#Robot settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "3Zzf6KDVpzAP"
   },
   "outputs": [],
   "source": [
    "rng = random.default_rng(seed = 42)\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, env, actions, survive, distanceMeasure):\n",
    "        self.agentX = 0\n",
    "        self.agentY = 0\n",
    "        self.env = env\n",
    "        self.totalActions = len(actions)\n",
    "        self.totalFeatures = 12 #FEATURES\n",
    "        self.actions = actions\n",
    "\n",
    "        self.coordinates = []\n",
    "        self.numVisitsMap = [[0 for i in range(self.env.discreteMap)] for j in range(self.env.discreteMap)]\n",
    "        self.numVisitsMap_accumulated = [[0 for i in range(self.env.discreteMap)] for j in range(self.env.discreteMap)]\n",
    "        \n",
    "        self.initialPositions = []\n",
    "\n",
    "        self.timeVisitsRechargeArea = []\n",
    "        self.time = 0\n",
    "        self.visitsRechargeAreaEpisode = []\n",
    "        self.numVisitsRechargeArea = [0 for i in range(len(env.rechargeAreas))]\n",
    "\n",
    "        self.distanceMeasure = distanceMeasure\n",
    "        \n",
    "        #BATTERY\n",
    "        self.FOV = survive['FOV']\n",
    "        self.homeostasisSurvive = survive['homeostasisSurvive']\n",
    "        self.maxEnergy = survive['maxEnergy']\n",
    "        self.minEnergy = survive['minEnergy']\n",
    "        self.discountEnergy = survive['discountEnergy']\n",
    "        self.energy = rng.integers(self.minEnergy, self.maxEnergy + 1)\n",
    "       \n",
    "        self.surviveDriveAll = []\n",
    "        self.energyAll = []\n",
    "        \n",
    "        self.allY_test = [0, 0, 0, 0, 1, 1, 1, 2, 2, 3, 3, 3, 3, 4, 5, 5, 5, 6, 6, 6, 7, 7, 7, 8, 8, 9, 9, 9, 10, 10, 11, 12, 12, 13, 13, 13, 14, 14, 15, 16, 16, 16, 17, 17, 18, 19, 19, 19, 19, 19]\n",
    "        self.allX_test = [0, 4, 6, 19, 8, 12, 15, 3, 10, 1, 6, 12, 16, 18, 2, 5, 7, 4, 10, 13, 0, 15, 19, 7, 14, 5, 12, 16, 2, 10, 19, 1, 13, 5, 8, 16, 0, 3, 10, 1, 4, 17, 9, 13, 7, 0, 3, 11, 16, 19]\n",
    "        self.cxy = 0\n",
    "\n",
    "        self.firstEpisode = True\n",
    "        \n",
    "        \n",
    "    def reset(self, test):\n",
    "        self.numVisitsRechargeArea = [0 for i in range(len(self.env.rechargeAreas))]\n",
    "       \n",
    "        if(test):\n",
    "            self.energy = self.homeostasisSurvive\n",
    "            self.agentX = self.allX_test[self.cxy]\n",
    "            self.agentY = self.allY_test[self.cxy]\n",
    "            self.cxy += 1\n",
    "            self.energyAll = []\n",
    "            self.surviveDriveAll = []\n",
    "        else:\n",
    "            self.energy = rng.integers(self.minEnergy, self.maxEnergy + 1)\n",
    "            self.agentX = rng.integers(0, self.env.discreteMap)\n",
    "            self.agentY = rng.integers(0, self.env.discreteMap)\n",
    "            \n",
    "        \n",
    "        self.energyAll.append(self.energy)\n",
    "        currentDriveSurvive = self.computeDriveSurvive(self.energy)\n",
    "        \n",
    "        self.initialPositions.append([self.agentY, self.agentX])\n",
    "        self.coordinates = []\n",
    "        self.numVisitsMap = [[0 for i in range(self.env.discreteMap)] for j in range(self.env.discreteMap)]\n",
    "        \n",
    "        if(self.firstEpisode == True):\n",
    "            self.firstEpisode = False\n",
    "        else:\n",
    "            self.timeVisitsRechargeArea.append(self.visitsRechargeAreaEpisode)\n",
    "\n",
    "        self.time = 0\n",
    "        self.visitsRechargeAreaEpisode = []\n",
    "        \n",
    "        minDist, up, down, left, right, station = self.closestRechargeArea(self.agentY, self.agentX)\n",
    "        reICanSee = self.RechargeArea_ICanSee(self.agentY, self.agentX)\n",
    "        \n",
    "        return [currentDriveSurvive, minDist, up, down, left, right, reICanSee, self.agentY, self.agentX] #FEATURES\n",
    "\n",
    "    \n",
    "    #DRIVE SURVIVE\n",
    "    def computeDriveSurvive(self, sensor):\n",
    "        return -(self.homeostasisSurvive - sensor)\n",
    "\n",
    "    #REWARD FUNCTIONS     \n",
    "    def rewardFunction(self, currentDriveSurvive):\n",
    "        if ((abs(int(currentDriveSurvive)) == 0) and (currentDriveSurvive * (-1) <= 0)):\n",
    "            return 1\n",
    "        elif (currentDriveSurvive < 0):\n",
    "            return currentDriveSurvive \n",
    "        else:\n",
    "            return -(currentDriveSurvive * 0.5) \n",
    "\n",
    "    def computeDriveSurviveReward(self, currentDriveSurvive):\n",
    "        return self.rewardFunction(currentDriveSurvive)\n",
    "\n",
    "    def step(self, action):\n",
    "        #MOVE\n",
    "        # Up \n",
    "        if (action == 1):\n",
    "            self.agentY += 1\n",
    "            if self.agentY > self.env.discreteMap-1: self.agentY = self.env.discreteMap-1\n",
    "\n",
    "        # Down\n",
    "        elif (action == 2):\n",
    "            self.agentY -= 1\n",
    "            if self.agentY < 0: self.agentY = 0\n",
    "\n",
    "        # Left\n",
    "        elif (action == 3):\n",
    "            self.agentX -=1\n",
    "            if self.agentX < 0: self.agentX = 0\n",
    "\n",
    "        # Right \n",
    "        elif (action == 4):\n",
    "            self.agentX += 1\n",
    "            if self.agentX > self.env.discreteMap-1: self.agentX = self.env.discreteMap-1\n",
    "\n",
    "        self.coordinates.append([self.agentX, self.agentY])\n",
    "        self.numVisitsMap[self.agentY][self.agentX] += 1\n",
    "\n",
    "        self.numVisitsMap_accumulated[self.agentY][self.agentX] += 1\n",
    "\n",
    "\n",
    "        #UPDATE BATTERY STUFF\n",
    "        self.energy -= self.discountEnergy\n",
    "\n",
    "        recharge = self.checkIfRecharge()#Recharge\n",
    "        self.energy = min(self.maxEnergy, self.energy + recharge)\n",
    "        self.energyAll.append(self.energy)\n",
    "\n",
    "        #Compute drives and reward\n",
    "        currentDriveSurvive = self.computeDriveSurvive(self.energy)\n",
    "        rewardSurvive = self.computeDriveSurviveReward(currentDriveSurvive)\n",
    "        self.surviveDriveAll.append(currentDriveSurvive)\n",
    "\n",
    "        totalReward = rewardSurvive\n",
    "\n",
    "        done = self.death()\n",
    "\n",
    "        minDist, up, down, left, right, station = self.closestRechargeArea(self.agentY, self.agentX)\n",
    "        reICanSee = self.RechargeArea_ICanSee(self.agentY, self.agentX)\n",
    "        \n",
    "        new_state = [currentDriveSurvive, minDist, up, down, left, right, reICanSee, self.agentY, self.agentX]\n",
    "        \n",
    "        self.time += 1\n",
    "        \n",
    "        return new_state, totalReward, done\n",
    "    \n",
    "\n",
    "    def death(self):\n",
    "        if self.energy <= self.minEnergy:\n",
    "            return 1\n",
    "        return 0\n",
    "\n",
    "    def checkIfRecharge(self):\n",
    "        recharge = self.env.ifInRechargeArea(self.agentY, self.agentX, self.numVisitsRechargeArea, self.visitsRechargeAreaEpisode, self.time)\n",
    "        return recharge\n",
    "    \n",
    "    def manhattanDistance(self, agentY, agentX, middleY_RA, middleX_RA):\n",
    "        return abs(agentX - middleX_RA) + abs(agentY - middleY_RA)\n",
    "\n",
    "    def euclideanDistance(self, agentY, agentX, middleY_RA, middleX_RA):\n",
    "        return math.sqrt(((agentX - middleX_RA) ** 2) + ((agentY - middleY_RA) ** 2))\n",
    "\n",
    "    def closestRechargeArea(self, agentY, agentX):\n",
    "        minDist = 99999\n",
    "        station = 0\n",
    "        i = 0\n",
    "        \n",
    "        for rA in self.env.rechargeAreas:\n",
    "            middleX_RA = rA[1] + self.env.sizeRechargeAreas/2\n",
    "            middleY_RA = rA[0] + self.env.sizeRechargeAreas/2\n",
    "            if (self.distanceMeasure == 'Euclidean'):\n",
    "                dist = self.euclideanDistance(agentY, agentX, middleY_RA, middleX_RA)\n",
    "            else:\n",
    "                dist = self.manhattanDistance(agentY, agentX, middleY_RA, middleX_RA)\n",
    "            \n",
    "\n",
    "            if ((self.distanceMeasure == 'Manhattan' and dist <= self.env.sizeRechargeAreas) or (self.distanceMeasure == 'Euclidean' and dist < self.env.sizeRechargeAreas)):#fiz isso pq to usando o ponto central da estação, mas se dist for menor que sizerecharge quer dizer que ja esta na estação (mas nao no meio dela)\n",
    "                dist = 0\n",
    "\n",
    "            if dist < minDist:\n",
    "                minDist = dist\n",
    "                up = 0\n",
    "                down = 0\n",
    "                left = 0\n",
    "                right = 0\n",
    "                station = i\n",
    "\n",
    "                horizon = agentX - middleX_RA \n",
    "                vertical = agentY - middleY_RA\n",
    "\n",
    "                if(agentX < rA[1] and (horizon != self.env.sizeRechargeAreas/2)):\n",
    "                    right = 1\n",
    "                elif(agentX > rA[1] and (horizon != self.env.sizeRechargeAreas/2)):\n",
    "                    left = 1\n",
    "\n",
    "                if(agentY > rA[0] and (vertical != self.env.sizeRechargeAreas/2)):\n",
    "                    down = 1\n",
    "                elif(agentY < rA[0] and (vertical != self.env.sizeRechargeAreas/2)):\n",
    "                    up = 1\n",
    "                    \n",
    "            i += 1\n",
    "\n",
    "        return minDist, up, down, left, right, station\n",
    "    \n",
    "    def RechargeArea_ICanSee(self, agentY, agentX):\n",
    "        reICanSee = np.zeros(len(self.env.rechargeAreas))\n",
    "        i = 0\n",
    "        \n",
    "        for rA in self.env.rechargeAreas:\n",
    "            middleX_RA = rA[1] + self.env.sizeRechargeAreas/2\n",
    "            middleY_RA = rA[0] + self.env.sizeRechargeAreas/2\n",
    "            if (self.distanceMeasure == 'Euclidean'):\n",
    "                dist = self.euclideanDistance(agentY, agentX, middleY_RA, middleX_RA)\n",
    "            else:\n",
    "                dist = self.manhattanDistance(agentY, agentX, middleY_RA, middleX_RA)\n",
    "            \n",
    "            if ((self.distanceMeasure == 'Manhattan' and dist <= self.env.sizeRechargeAreas) or (self.distanceMeasure == 'Euclidean' and dist < self.env.sizeRechargeAreas)):#fiz isso pq to usando o ponto central da estação, mas se dist for menor que sizerecharge quer dizer que ja esta na estação (mas nao no meio dela)\n",
    "                dist = 0\n",
    "\n",
    "            if dist <= self.FOV:\n",
    "                reICanSee[i] = 1\n",
    "                \n",
    "            i+= 1\n",
    "\n",
    "        return reICanSee\n",
    "\n",
    "    def getRobot_Data(self):\n",
    "        minDist, up, down, left, right, station = self.closestRechargeArea(self.agentY, self.agentX)\n",
    "        return self.agentX, self.agentY, minDist, station, self.energy, self.discountEnergy, self.minEnergy, self.homeostasisSurvive\n",
    "    \n",
    "    def save(self, sensor, fileName):\n",
    "        mat = np.matrix(sensor)\n",
    "        with open(fileName,'wb') as f:\n",
    "            for line in mat:\n",
    "                np.savetxt(f, line, fmt='%s', delimiter=\",\")\n",
    "        f.close()\n",
    "\n",
    "    def saveSensorsData(self, expIDX):\n",
    "        self.save(self.energyAll, 'Energy' + str(expIDX) + '.txt')\n",
    "        self.save(self.surviveDriveAll, 'SurviveDrive' + str(expIDX) + '.txt')\n",
    "        self.save(self.numVisitsRechargeArea, 'NumVisitsRechargeArea' + str(expIDX) + '.txt')\n",
    "        self.save(self.numVisitsMap, 'NumVisitsMap_Test' + str(expIDX) + '.txt')\n",
    "        self.save(self.coordinates, 'Coordinates' + str(expIDX) + '.txt')\n",
    "        \n",
    "\n",
    "    def dataTrain_Visits(self):\n",
    "        self.save(self.numVisitsMap_accumulated, 'NumVisitsMap_TrainAccumulated0.txt')\n",
    "        \n",
    "    def saveTrain_InitialPosition(self):\n",
    "        with open('Train_InitialPositions.txt','wb') as f:\n",
    "            np.savetxt(f, self.initialPositions, fmt='%s', delimiter=\",\")\n",
    "        f.close()\n",
    "\n",
    "    def saveRechargeAreaVisits(self, fileName):\n",
    "        with open(fileName + '.txt', 'wb') as fp:\n",
    "            pickle.dump(self.timeVisitsRechargeArea, fp)\n",
    "        fp.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "SJDF1onT0Ryx"
   },
   "source": [
    "#Q-Learning Function Approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "YgZsy86tz32o"
   },
   "outputs": [],
   "source": [
    "rng = random.default_rng(seed = 42)\n",
    "\n",
    "class ApproximateQAgent:\n",
    "    def __init__(self, robot, learning_parameters, exploration_parameters, glie):\n",
    "        self.robot = robot \n",
    "\n",
    "        # learning parameters (dict)\n",
    "        self.alpha = learning_parameters['alpha']#learning rate\n",
    "        self.gamma = learning_parameters['gamma'] #discount factor\n",
    "\n",
    "        # exploration parameters\n",
    "        self.epsilon = exploration_parameters['epsilon']\n",
    "        self.epsilon_min = exploration_parameters['epsilon_min']\n",
    "        self.epsilon_decay = exploration_parameters['epsilon_decay']\n",
    "        self.glie = glie\n",
    "\n",
    "        self.featuresS = 0\n",
    "        self.featuresSL = 0\n",
    "\n",
    "        self.featuresPerAction = 0\n",
    "        \n",
    "        self.stationFeatures  = len(self.robot.env.rechargeAreas)\n",
    "        self.independFeatures = self.robot.totalFeatures - self.stationFeatures - 2 #All features, except SeeA, SeeB, SeeC, SeeD, X and Y\n",
    "\n",
    "################################## FILES #################################################\n",
    "\n",
    "    def recoverWeights(self):\n",
    "        with open('FeaturesPerAction_weightsLast.txt', 'r') as f:\n",
    "            self.featuresPerAction = [[float(num) for num in line.split(',')] for line in f]\n",
    "        f.close()\n",
    "\n",
    "    def saveWeights(self, fileName, data):\n",
    "        mat = np.matrix(data)\n",
    "        with open(fileName + '.txt','wb') as f:\n",
    "            for line in mat:\n",
    "                np.savetxt(f, line, fmt='%s', delimiter=\",\")\n",
    "        f.close()\n",
    "\n",
    "    def saveDataTraining(self, episode_rewards, episode_steps, filename):\n",
    "        z = zip(episode_rewards, episode_steps)\n",
    "        f = open(filename, 'w')\n",
    "        for t in z:\n",
    "            line = ' '.join(str(x) for x in t)\n",
    "            f.write(line + '\\n')\n",
    "        f.close()\n",
    "\n",
    "    def saveBestReward(self, bestEpisode, episode_rewards):\n",
    "        best = [bestEpisode, episode_rewards[bestEpisode]]\n",
    "        print(best)\n",
    "        with open('BestEpisodeReward.txt','wb') as f:\n",
    "            np.savetxt(f, best, fmt='%s', delimiter=\",\")\n",
    "        f.close()\n",
    "        \n",
    "    def saveInitialVars(self, minDist, up, down, left, right, station, agentX, agentY, energy, expIDX):\n",
    "        with open('InitialVars_Test' + str(expIDX) + '.txt','wb') as f:\n",
    "            np.savetxt(f, [minDist, up, down, left, right, station, agentX, agentY, energy], fmt='%s', delimiter=\",\")\n",
    "        f.close()\n",
    "        \n",
    "################################## Qlaerning #################################################\n",
    "\n",
    "    def init_featuresWeight(self):\n",
    "        self.featuresPerAction = [[rng.random() * 0.01 for i in range(self.independFeatures + self.stationFeatures + (self.robot.totalFeatures - self.independFeatures - self.stationFeatures) * self.robot.env.discreteMap)] for j in range(self.robot.totalActions)] \n",
    "   \n",
    "    def setFeatures_binario(self, stateFeatures_, state_S_or_SL):\n",
    "        size = self.robot.env.discreteMap\n",
    "        stateFeatures = np.zeros(self.independFeatures + self.stationFeatures + (self.robot.totalFeatures - self.independFeatures - self.stationFeatures) * self.robot.env.discreteMap)\n",
    "        \n",
    "        for i in range(self.independFeatures):\n",
    "            stateFeatures[i] = stateFeatures_[i]#currentDriveSurvive, minDist, up, down, left, right\n",
    "        \n",
    "        i += 1\n",
    "        k = 0\n",
    "        for j in range(i, i + self.stationFeatures):\n",
    "            stateFeatures[j] = stateFeatures_[i][k] #can see Recharge Area A, B, C, D\n",
    "            k += 1\n",
    "            \n",
    "        inter = np.zeros(size)\n",
    "        j = 0\n",
    "        for i in range(self.independFeatures + 1, len(stateFeatures_)):\n",
    "            inter[stateFeatures_[i]] = 1\n",
    "            stateFeatures[j * size + self.independFeatures + self.stationFeatures : (j+1) * size + self.independFeatures + self.stationFeatures] = inter\n",
    "            inter[stateFeatures_[i]] = 0\n",
    "            j += 1\n",
    "\n",
    "        if state_S_or_SL == 0:\n",
    "            self.featuresS = stateFeatures\n",
    "        else:\n",
    "            self.featuresSL = stateFeatures\n",
    "\n",
    "    def getFeatures(self, state_S_or_SL):\n",
    "        if state_S_or_SL == 0:\n",
    "            return self.featuresS\n",
    "        else:\n",
    "            return self.featuresSL\n",
    "\n",
    "    def getQvalue(self, actionIndex, state_S_or_SL):\n",
    "        qValue = 0\n",
    "        features = self.getFeatures(state_S_or_SL)\n",
    "\n",
    "        for i in range(self.independFeatures + self.stationFeatures + (self.robot.totalFeatures - self.independFeatures - self.stationFeatures) * self.robot.env.discreteMap):\n",
    "            qValue += features[i] * self.featuresPerAction[actionIndex][i]\n",
    "\n",
    "        return qValue\n",
    "  \n",
    "    def getMaxQValue(self):\n",
    "        maxQinSL = self.getQvalue(0, 1)#I assume that the first is the best\n",
    "        for i in range(1, self.robot.totalActions):\n",
    "            value = self.getQvalue(i, 1)\n",
    "            if value > maxQinSL:\n",
    "                maxQinSL = value\n",
    "        return maxQinSL\n",
    "\n",
    "    def update(self, actionIdid, reward):\n",
    "        Q_sa = self.getQvalue(actionIdid, 0)\n",
    "        Max_Qsl = self.getMaxQValue()\n",
    "        TD_target = reward + self.gamma * Max_Qsl\n",
    "        for i in range(self.independFeatures + self.stationFeatures + (self.robot.totalFeatures - self.independFeatures - self.stationFeatures) * self.robot.env.discreteMap):\n",
    "            self.featuresPerAction[actionIdid][i] += self.alpha * ((TD_target - Q_sa) * self.featuresS[i])\n",
    "\n",
    "        self.featuresS = self.featuresSL[:]\n",
    "\n",
    "    def updateEpsilon(self, totalEpisodes):\n",
    "        if self.glie == 'linear':\n",
    "            self.epsilon = max(self.epsilon_min, self.epsilon - (1/totalEpisodes))\n",
    "        elif self.glie == 'exponential':\n",
    "            self.epsilon = max(self.epsilon_min, self.epsilon * (1 - self.epsilon_decay))\n",
    "        elif self.glie == 'constant':\n",
    "            self.epsilon = self.epsilon\n",
    "\n",
    "    def getAction(self):\n",
    "        if rng.random() < self.epsilon:\n",
    "          # exploration, random choice\n",
    "            action = rng.integers(0,self.robot.totalActions)\n",
    "            selectedAction = action\n",
    "        else:\n",
    "          # exploitation, max value for given state\n",
    "            selectedAction = 0 #Assume that the first is the better\n",
    "            maxQinSL = self.getQvalue(0, 0)\n",
    "\n",
    "            for i in range(1, self.robot.totalActions):\n",
    "                value = self.getQvalue(i, 0)\n",
    "                if value > maxQinSL:\n",
    "                    maxQinSL = value\n",
    "                    selectedAction = i\n",
    "\n",
    "        return selectedAction\n",
    "\n",
    "################################## TRAIN #################################################\n",
    "\n",
    "    def learn(self, max_steps = 5000, total_episodes = 25000):\n",
    "        self.init_featuresWeight()\n",
    "        self.episode_rewards = np.zeros(total_episodes)\n",
    "        self.episode_steps = np.zeros(total_episodes) \n",
    "        self.discountedReward = np.zeros(total_episodes) \n",
    "        bestEpisode = 0\n",
    "        self.robotData_init = []\n",
    "        for episode in range(total_episodes):\n",
    "            state = self.robot.reset(0)\n",
    "            R_agentX, R_agentY, R_minDist, R_station, R_energy, R_discountEnergy, R_minEnergy, R_homeostasisSurvive = self.robot.getRobot_Data()\n",
    "            self.setFeatures_binario(state[:], 0)\n",
    "            for step in range(max_steps):\n",
    "                action = self.getAction()\n",
    "                new_state, reward, done = self.robot.step(action)\n",
    "                self.setFeatures_binario(new_state[:], 1)#setting features of state S'\n",
    "                self.update(action, reward)\n",
    "                self.episode_rewards[episode] += reward\n",
    "                self.discountedReward[episode] += reward * self.gamma ** step\n",
    "                if done:  \n",
    "                    print(\"died episode \", episode ,  \" \" , np.sum(self.robot.numVisitsRechargeArea), \"reward:\", self.episode_rewards[episode])\n",
    "                    break\n",
    "            if (done == 0):\n",
    "                print(\"episode \", episode ,  \" \" ,  np.sum(self.robot.numVisitsRechargeArea), \"reward:\", self.episode_rewards[episode])  \n",
    "            self.robotData_init.append([R_agentX, R_agentY, R_minDist, R_station, R_energy, R_discountEnergy, R_minEnergy, R_homeostasisSurvive, done])\n",
    "            self.episode_steps[episode] = step + 1\n",
    "            self.updateEpsilon(total_episodes)\n",
    "            if (self.episode_rewards[episode]/self.episode_steps[episode] >= self.episode_rewards[bestEpisode]/self.episode_steps[bestEpisode]):\n",
    "                self.saveWeights('FeaturesPerAction_weightsBEST', self.featuresPerAction)\n",
    "                bestEpisode = episode\n",
    "            if (episode % 1000 == 0):\n",
    "                self.saveDataTraining(self.episode_rewards, self.episode_steps, 'RewardsLearning.txt')\n",
    "                self.saveDataTraining(self.discountedReward, self.episode_steps, 'DiscountedRewardsLearning.txt')\n",
    "                self.saveBestReward(bestEpisode, self.episode_rewards)\n",
    "                self.robot.saveRechargeAreaVisits('TimeVisitsRechargeArea_Train')\n",
    "                self.robot.dataTrain_Visits()\n",
    "\n",
    "        state = self.robot.reset(0)#Isso ta aqui pra salvar o TimeVisitsRechargeArea do ultimo episodio\n",
    "        self.saveWeights('FeaturesPerAction_weightsLast', self.featuresPerAction)\n",
    "        self.saveDataTraining(self.episode_rewards, self.episode_steps, 'RewardsLearning.txt')\n",
    "        self.saveDataTraining(self.discountedReward, self.episode_steps, 'DiscountedRewardsLearning.txt')\n",
    "        self.robot.saveRechargeAreaVisits('TimeVisitsRechargeArea_Train')\n",
    "        self.saveBestReward(bestEpisode, self.episode_rewards)\n",
    "        self.robot.dataTrain_Visits()\n",
    "        self.robot.saveTrain_InitialPosition()\n",
    "        np.savetxt('robotData_init.txt', self.robotData_init, fmt='%.2f')\n",
    "################################## TEST #################################################\n",
    "\n",
    "    def evaluate(self, max_steps, expIDX):\n",
    "        self.epsilon = 0\n",
    "        self.recoverWeights()\n",
    "        state = self.robot.reset(1)\n",
    "        self.reward_want = []\n",
    "        self.reward_like = []\n",
    "        minDist, up, down, left, right, station = self.robot.closestRechargeArea(self.robot.agentY, self.robot.agentX)\n",
    "        self.saveInitialVars(minDist, up, down, left, right, station, self.robot.agentX, self.robot.agentY, self.robot.energy, expIDX)\n",
    "        self.setFeatures_binario(state[:], 0)\n",
    "        for step in range(max_steps):\n",
    "            action = self.getAction()\n",
    "            new_state, reward, done = self.robot.step(action)\n",
    "            self.setFeatures_binario(new_state[:], 0)\n",
    "            if done:\n",
    "                print(\"died EXPLORING \" , step, \" steps\")\n",
    "                print(np.sum(self.robot.numVisitsRechargeArea))\n",
    "                break\n",
    "        self.robot.saveSensorsData(expIDX)\n",
    "        self.robot.saveRechargeAreaVisits('TimeVisitsRechargeArea_Test')\n",
    "        \n",
    "################################## ONLY ANALYZE ACTION CHOSE #################################################\n",
    "\n",
    "    def chooseActionPerPosition(self,energy):\n",
    "        self.epsilon = 0\n",
    "        self.recoverWeights()\n",
    "\n",
    "        actions = [[-1  for i in range(self.robot.env.discreteMap)] for j in range(self.robot.env.discreteMap)]\n",
    "\n",
    "        energyDrive = self.robot.computeDriveSurvive(energy)\n",
    "\n",
    "        for i in range(self.robot.env.discreteMap):\n",
    "            for j in range(self.robot.env.discreteMap):\n",
    "                minDist, up, down, left, right, station = self.robot.closestRechargeArea(i, j)\n",
    "                reICanSee = self.robot.RechargeArea_ICanSee(i, j)\n",
    "                state = [energyDrive, minDist, up, down, left, right, reICanSee, i, j]\n",
    "                self.setFeatures_binario(state[:], 0)\n",
    "                action = self.getAction()\n",
    "                actions[i][j] = action\n",
    "        self.saveWeights('ActionsPerPosition_Energy' + str(energy), actions)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "_tY0Lp-6qH3w"
   },
   "source": [
    "#Experiments"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "KoK7D75MqRyW"
   },
   "source": [
    "##Start the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "7tTi1XqLqQ7Y"
   },
   "outputs": [],
   "source": [
    "settings = {\n",
    "    'discreteMap': 20,\n",
    "    'sizeRechargeAreas': 1,\n",
    "    'labelsRechargeArea': ['A', 'B', 'C', 'D'],\n",
    "    'rechargeAreas': [[2,4], [4,13], [16, 2], [14,15]],#[yInicial, XInicial]\n",
    "    'rechargeValue': [1, 4, 3, 2]#[1, 4, 3, 2] #[3, 3, 3, 3] #\n",
    "}\n",
    "\n",
    "env = Environment(settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "B4JzeSYH2uiF"
   },
   "outputs": [],
   "source": [
    "#Need: Energy, Drive: Survive\n",
    "survive = {\n",
    "    'homeostasisSurvive': 30,\n",
    "    'maxEnergy': 50,\n",
    "    'minEnergy': 0,\n",
    "    'discountEnergy': 0.1,\n",
    "    'FOV': 6\n",
    "}\n",
    "\n",
    "actions = {\n",
    "    0: 'Stop',\n",
    "    1: 'Up',\n",
    "    2: 'Down',\n",
    "    3: 'Left',\n",
    "    4: 'Right'\n",
    "}\n",
    "\n",
    "distanceMeasure = ['Euclidean', 'Manhattan']\n",
    "robot = Agent(env, actions, survive, distanceMeasure[1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "Ih0X8udE08G1"
   },
   "outputs": [],
   "source": [
    "# q-learning parameters\n",
    "learning_parameters = {\n",
    "    'alpha': 0.0001,\n",
    "    'gamma': 0.9\n",
    "} \n",
    "# exploration-exploitation parameters\n",
    "exploration_parameters = {\n",
    "    'epsilon': 1.0, #exploration probability at start\n",
    "    'epsilon_min': 0.01, #minimum exploration probability\n",
    "    'epsilon_decay': 0.0003  #exponential decay rate for exploration prob\n",
    "}\n",
    "\n",
    "glie = ['linear', 'exponential', 'constant']\n",
    "qApp_Agent_1 = ApproximateQAgent(robot, learning_parameters, exploration_parameters, glie[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sdovEZ6LA_Q2"
   },
   "outputs": [],
   "source": [
    "max_steps = 5000\n",
    "total_episodes = 25000\n",
    "qApp_Agent_1.learn(max_steps, total_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6iOvqdIWlSMQ"
   },
   "outputs": [],
   "source": [
    "max_steps = 8000\n",
    "numExps = 50\n",
    "for i in range(numExps):\n",
    "    qApp_Agent_1.evaluate(max_steps, i)\n",
    "    print('End of the experiment ', i)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Energy_Learner.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
